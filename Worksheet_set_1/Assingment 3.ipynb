{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb32495d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Write a python program which searches all the product under a particular product from www.amazon.in.\n",
    "# The product to be searched will be taken as input from user. For e.g. If user input is ‘guitar’. Then search\n",
    "# for guitars.\n",
    "# 2. In the above question, now scrape the following details of each product listed in first 3 pages of your\n",
    "# search results and save it in a data frame and csv. In case if any product has less than 3 pages in search\n",
    "# results then scrape all the products available under that product name. Details to be scraped are: \"Brand\n",
    "# Name\", \"Name of the Product\", \"Price\", \"Return/Exchange\", \"Expected Delivery\", \"Availability\" and\n",
    "# “Product URL”. In case, if any of the details are missing for any of the product then replace it by “-“\n",
    "\n",
    "# importing libraries\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "def main(URL):\n",
    "\t# opening our output file in append mode\n",
    "\tFile = open(\"out.csv\", \"a\")\n",
    "\n",
    "\t# specifying user agent, You can use other user agents\n",
    "\t# available on the internet\n",
    "\tHEADERS = ({'User-Agent':'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/44.0.2403.157 Safari/537.36', 'Accept-Language': 'en-US, en;q=0.5'})\n",
    "\n",
    "\t# Making the HTTP Request\n",
    "\twebpage = requests.get(URL, headers=HEADERS)\n",
    "\n",
    "\t# Creating the Soup Object containing all data\n",
    "\tsoup = BeautifulSoup(webpage.content, \"lxml\")\n",
    "\n",
    "\t# retrieving product title\n",
    "\ttry:\n",
    "\t\t# Outer Tag Object\n",
    "\t\ttitle = soup.find(\"span\",\n",
    "\t\t\t\t\t\tattrs={\"id\": 'productTitle'})\n",
    "\n",
    "\t\t# Inner NavigableString Object\n",
    "\t\ttitle_value = title.string\n",
    "\n",
    "\t\t# Title as a string value\n",
    "\t\ttitle_string = title_value.strip().replace(',', '')\n",
    "\n",
    "\texcept AttributeError:\n",
    "\t\ttitle_string = \"NA\"\n",
    "\tprint(\"product Title = \", title_string)\n",
    "\n",
    "\t# saving the title in the file\n",
    "\tFile.write(f\"{title_string},\")\n",
    "\n",
    "\t# retrieving price\n",
    "\ttry:\n",
    "\t\tprice = soup.find(\n",
    "\t\t\t\"span\", attrs={'id': 'priceblock_ourprice'})\n",
    "\t\t\t\t\t\t\t\t.string.strip().replace(',', '')\n",
    "\t\t# we are omitting unnecessary spaces\n",
    "\t\t# and commas form our string\n",
    "\texcept AttributeError:\n",
    "\t\tprice = \"NA\"\n",
    "\tprint(\"Products price = \", price)\n",
    "\n",
    "\t# saving\n",
    "\tFile.write(f\"{price},\")\n",
    "\n",
    "\t# retrieving product rating\n",
    "\ttry:\n",
    "\t\trating = soup.find(\"i\", attrs={\n",
    "\t\t\t\t\t\t'class': 'a-icon a-icon-star a-star-4-5'})\n",
    "\t\t\t\t\t\t\t\t\t.string.strip().replace(',', '')\n",
    "\n",
    "\texcept AttributeError:\n",
    "\n",
    "\t\ttry:\n",
    "\t\t\trating = soup.find(\n",
    "\t\t\t\t\"span\", attrs={'class': 'a-icon-alt'})\n",
    "\t\t\t\t\t\t\t\t.string.strip().replace(',', '')\n",
    "\t\texcept:\n",
    "\t\t\trating = \"NA\"\n",
    "\tprint(\"Overall rating = \", rating)\n",
    "\n",
    "\tFile.write(f\"{rating},\")\n",
    "\n",
    "\ttry:\n",
    "\t\treview_count = soup.find(\n",
    "\t\t\t\"span\", attrs={'id': 'acrCustomerReviewText'})\n",
    "\t\t\t\t\t\t\t\t.string.strip().replace(',', '')\n",
    "\n",
    "\texcept AttributeError:\n",
    "\t\treview_count = \"NA\"\n",
    "\tprint(\"Total reviews = \", review_count)\n",
    "\tFile.write(f\"{review_count},\")\n",
    "\n",
    "\t# print availablility status\n",
    "\ttry:\n",
    "\t\tavailable = soup.find(\"div\", attrs={'id': 'availability'})\n",
    "\t\tavailable = available.find(\"span\")\n",
    "\t\t\t\t\t.string.strip().replace(',', '')\n",
    "\n",
    "\texcept AttributeError:\n",
    "\t\tavailable = \"NA\"\n",
    "\tprint(\"Availability = \", available)\n",
    "\n",
    "\t# saving the availability and closing the line\n",
    "\tFile.write(f\"{available},\\n\")\n",
    "\n",
    "\t# closing the file\n",
    "\tFile.close()\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "# opening our url file to access URLs\n",
    "\tfile = open(\"url.txt\", \"r\")\n",
    "\n",
    "\t# iterating over the urls\n",
    "\tfor links in file.readlines():\n",
    "\t\tmain(links)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Write a python program to access the search bar and search button on images.google.com and scrape 10\n",
    "# images each for keywords ‘fruits’, ‘cars’ and ‘Machine Learning’, ‘Guitar’, ‘Cakes’.\n",
    "\n",
    "import os\n",
    "import time\n",
    "\n",
    "import io\n",
    "import hashlib\n",
    "import signal\n",
    "from glob import glob\n",
    "import requests\n",
    "\n",
    "from PIL import Image\n",
    "from selenium import webdriver\n",
    "\n",
    "number_of_images = 400\n",
    "GET_IMAGE_TIMEOUT = 2\n",
    "SLEEP_BETWEEN_INTERACTIONS = 0.1\n",
    "SLEEP_BEFORE_MORE = 5\n",
    "IMAGE_QUALITY = 85\n",
    "\n",
    "output_path = \"/path/to/your/image/directory\"\n",
    "\n",
    "search_terms = [\n",
    "    \"fruits\",\n",
    "    \"cars\",\n",
    "    \"Machine Learning\",\n",
    "    \"Guitar\",\n",
    "    \"Cakes\",\n",
    "]\n",
    "\n",
    "dirs = glob(output_path + \"*\")\n",
    "dirs = [dir.split(\"/\")[-1].replace(\"_\", \" \") for dir in dirs]\n",
    "search_terms = [term for term in search_terms if term not in dirs]\n",
    "\n",
    "wd = webdriver.Chrome()\n",
    "wd.get(\"https://google.com\")\n",
    "\n",
    "class timeout:\n",
    "    def __init__(self, seconds=1, error_message=\"Timeout\"):\n",
    "        self.seconds = seconds\n",
    "        self.error_message = error_message\n",
    "\n",
    "    def handle_timeout(self, signum, frame):\n",
    "        raise TimeoutError(self.error_message)\n",
    "\n",
    "    def __enter__(self):\n",
    "        signal.signal(signal.SIGALRM, self.handle_timeout)\n",
    "        signal.alarm(self.seconds)\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        signal.alarm(0)\n",
    "        \n",
    "    def fetch_image_urls(\n",
    "    query: str,\n",
    "    max_links_to_fetch: int,\n",
    "    wd: webdriver,\n",
    "    sleep_between_interactions: int = 1,\n",
    "):\n",
    "    def scroll_to_end(wd):\n",
    "        wd.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(sleep_between_interactions)\n",
    "\n",
    "    # Build the Google Query.\n",
    "    search_url = \"https://www.google.com/search?safe=off&site=&tbm=isch&source=hp&q={q}&oq={q}&gs_l=img\"\n",
    "\n",
    "    # load the page\n",
    "    wd.get(search_url.format(q=query))\n",
    "\n",
    "    # Declared as a set, to prevent duplicates.\n",
    "    image_urls = set()\n",
    "    image_count = 0\n",
    "    results_start = 0\n",
    "    while image_count < max_links_to_fetch:\n",
    "        scroll_to_end(wd)\n",
    "\n",
    "        # Get all image thumbnail results\n",
    "        thumbnail_results = wd.find_elements_by_css_selector(\"img.Q4LuWd\")\n",
    "        number_results = len(thumbnail_results)\n",
    "\n",
    "        print(\n",
    "            f\"Found: {number_results} search results. Extracting links from {results_start}:{number_results}\"\n",
    "        )\n",
    "\n",
    "        # Loop through image thumbnail identified\n",
    "        for img in thumbnail_results[results_start:number_results]:\n",
    "            # Try to click every thumbnail such that we can get the real image behind it.\n",
    "            try:\n",
    "                img.click()\n",
    "                time.sleep(sleep_between_interactions)\n",
    "            except Exception:\n",
    "                continue\n",
    "\n",
    "            # Extract image urls\n",
    "            actual_images = wd.find_elements_by_css_selector(\"img.n3VNCb\")\n",
    "            for actual_image in actual_images:\n",
    "                if actual_image.get_attribute(\n",
    "                    \"src\"\n",
    "                ) and \"http\" in actual_image.get_attribute(\"src\"):\n",
    "                    image_urls.add(actual_image.get_attribute(\"src\"))\n",
    "\n",
    "            image_count = len(image_urls)\n",
    "\n",
    "            # If the number images found exceeds our `num_of_images`, end the seaerch.\n",
    "            if len(image_urls) >= max_links_to_fetch:\n",
    "                print(f\"Found: {len(image_urls)} image links, done!\")\n",
    "                break\n",
    "        else:\n",
    "            # If we haven't found all the images we want, let's look for more.\n",
    "            print(\"Found:\", len(image_urls), \"image links, looking for more ...\")\n",
    "            time.sleep(SLEEP_BEFORE_MORE)\n",
    "\n",
    "            # Check for button signifying no more images.\n",
    "            not_what_you_want_button = \"\"\n",
    "            try:\n",
    "                not_what_you_want_button = wd.find_element_by_css_selector(\".r0zKGf\")\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            # If there are no more images return.\n",
    "            if not_what_you_want_button:\n",
    "                print(\"No more images available.\")\n",
    "                return image_urls\n",
    "\n",
    "            # If there is a \"Load More\" button, click it.\n",
    "            load_more_button = wd.find_element_by_css_selector(\".mye4qd\")\n",
    "            if load_more_button and not not_what_you_want_button:\n",
    "                wd.execute_script(\"document.querySelector('.mye4qd').click();\")\n",
    "\n",
    "        # Move the result startpoint further down.\n",
    "        results_start = len(thumbnail_results)\n",
    "\n",
    "    return image_urls\n",
    "\n",
    "\n",
    "def persist_image(folder_path: str, url: str):\n",
    "    try:\n",
    "        print(\"Getting image\")\n",
    "        # Download the image.  If timeout is exceeded, throw an error.\n",
    "        with timeout(GET_IMAGE_TIMEOUT):\n",
    "            image_content = requests.get(url).content\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not download {url} - {e}\")\n",
    "\n",
    "    try:\n",
    "        # Convert the image into a bit stream, then save it.\n",
    "        image_file = io.BytesIO(image_content)\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "        # Create a unique filepath from the contents of the image.\n",
    "        file_path = os.path.join(\n",
    "            folder_path, hashlib.sha1(image_content).hexdigest()[:10] + \".jpg\"\n",
    "        )\n",
    "        with open(file_path, \"wb\") as f:\n",
    "            image.save(f, \"JPEG\", quality=IMAGE_QUALITY)\n",
    "        print(f\"SUCCESS - saved {url} - as {file_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR - Could not save {url} - {e}\")\n",
    "\n",
    "def search_and_download(search_term: str, target_path=\"./images\", number_images=5):\n",
    "    # Create a folder name.\n",
    "    target_folder = os.path.join(target_path, \"_\".join(search_term.lower().split(\" \")))\n",
    "\n",
    "    # Create image folder if needed.\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "\n",
    "    # Open Chrome\n",
    "    with webdriver.Chrome() as wd:\n",
    "        # Search for images URLs.\n",
    "        res = fetch_image_urls(\n",
    "            search_term,\n",
    "            number_images,\n",
    "            wd=wd,\n",
    "            sleep_between_interactions=SLEEP_BETWEEN_INTERACTIONS,\n",
    "        )\n",
    "\n",
    "        # Download the images.\n",
    "        if res is not None:\n",
    "            for elem in res:\n",
    "                persist_image(target_folder, elem)\n",
    "        else:\n",
    "            print(f\"Failed to return links for term: {search_term}\")\n",
    "\n",
    "# Loop through all the search terms.\n",
    "for term in search_terms:\n",
    "    search_and_download(term, output_path, number_of_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff8dec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Write a python program to search for a smartphone(e.g.: Oneplus Nord, pixel 4A, etc.) on www.flipkart.com and scrape following details for all the search results displayed on 1st page. Details to be scraped: “Brand Name”, “Smartphone name”, “Colour”, “RAM”, “Storage(ROM)”, “Primary Camera”, “Secondary Camera”, “Display Size”, “Battery Capacity”, “Price”, “Product URL”. Incase if any of the details is missing then replace it by “- “. Save your results in a dataframe and CSV.\n",
    "\n",
    "\n",
    "from selenium import webdriver\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome(\"/usr/lib/chromium-browser/chromedriver\")\n",
    "\n",
    "products=[] #List to store name of the product\n",
    "prices=[] #List to store price of the product\n",
    "ratings=[] #List to store rating of the product\n",
    "driver.get(\"https://www.flipkart.com/search?q=pixel+4a&sid=tyy%2C4io&as=on&as-show=on&otracker=AS_QueryStore_OrganicAutoSuggest_1_5_na_na_na&otracker1=AS_QueryStore_OrganicAutoSuggest_1_5_na_na_na&as-pos=1&as-type=HISTORY&suggestionId=pixel+4a%7CMobiles&requestId=737ec53e-7da5-424c-9eea-815cd8befbe1\")\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)\n",
    "for a in soup.findAll('a',href=True, attrs={'class':'_31qSD5'}):\n",
    "name=a.find('div', attrs={'class':'_3wU53n'})\n",
    "price=a.find('div', attrs={'class':'_1vC4OE _2rQ-NK'})\n",
    "rating=a.find('div', attrs={'class':'hGSR34 _2beYZw'})\n",
    "products.append(name.text)\n",
    "prices.append(price.text)\n",
    "ratings.append(rating.text)\n",
    "\n",
    "python web-s.py\n",
    "df = pd.DataFrame({'Product Name':products,'Price':prices,'Rating':ratings})df.to_csv('products.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5093d25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Write a program to scrap geospatial coordinates (latitude, longitude) of a city searched on google maps.\n",
    "\n",
    "# import required modules\n",
    "from selenium import webdriver\n",
    "from time import sleep\n",
    "\n",
    "\n",
    "# assign url in the webdriver object\n",
    "driver = webdriver.Chrome()\n",
    "driver.get(\"https://www.google.co.in/maps/@10.8091781,78.2885026,7z\")\n",
    "sleep(2)\n",
    "\n",
    "\n",
    "# search locations\n",
    "def searchplace():\n",
    "\tPlace = driver.find_element_by_class_name(\"tactile-searchbox-input\")\n",
    "\tPlace.send_keys(\"Tiruchirappalli\")\n",
    "\tSubmit = driver.find_element_by_xpath(\n",
    "\t\t\"/html/body/jsl/div[3]/div[9]/div[3]/div[1]/div[1]/div[1]/div[2]/div[1]/button\")\n",
    "\tSubmit.click()\n",
    "\n",
    "searchplace()\n",
    "\n",
    "\n",
    "# get directions\n",
    "def directions():\n",
    "\tsleep(10)\n",
    "\tdirections = driver.find_element_by_xpath(\n",
    "\t\t\"/html/body/jsl/div[3]/div[9]/div[7]/div/div[1]/div/div/div[5]/div[1]/div/button\")\n",
    "\tdirections.click()\n",
    "\n",
    "directions()\n",
    "\n",
    "\n",
    "# find place\n",
    "def find():\n",
    "\tsleep(6)\n",
    "\tfind = driver.find_element_by_xpath(\n",
    "\t\t\"/html/body/jsl/div[3]/div[9]/div[3]/div[1]/div[2]/div/div[3]/div[1]/div[1]/div[2]/div/div/input\")\n",
    "\tfind.send_keys(\"Tirunelveli\")\n",
    "\tsleep(2)\n",
    "\tsearch = driver.find_element_by_xpath(\n",
    "\t\t\"/html/body/jsl/div[3]/div[9]/div[3]/div[1]/div[2]/div/div[3]/div[1]/div[1]/div[2]/button[1]\")\n",
    "\tsearch.click()\n",
    "\n",
    "find()\n",
    "\n",
    "\n",
    "# get transportation details\n",
    "def kilometers():\n",
    "\tsleep(5)\n",
    "\tTotalkilometers = driver.find_element_by_xpath(\n",
    "\t\t\"/html/body/jsl/div[3]/div[9]/div[7]/div/div[1]/div/div/div[5]/div[1]/div[1]/div[1]/div[1]/div[2]/div\")\n",
    "\tprint(\"Total Kilometers:\", Totalkilometers.text)\n",
    "\tsleep(5)\n",
    "\tBus = driver.find_element_by_xpath(\n",
    "\t\t\"/html/body/jsl/div[3]/div[9]/div[7]/div/div[1]/div/div/div[5]/div[1]/div[1]/div[1]/div[1]/div[1]/span[1]\")\n",
    "\tprint(\"Bus Travel:\", Bus.text)\n",
    "\tsleep(7)\n",
    "\tTrain = driver.find_element_by_xpath(\n",
    "\t\t\"/html/body/jsl/div[3]/div[9]/div[7]/div/div[1]/div/div/div[5]/div[2]/div[1]/div[2]/div[1]/div\")\n",
    "\tprint(\"Train Travel:\", Train.text)\n",
    "\tsleep(7)\n",
    "\n",
    "kilometers()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e6422",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Write a program to scrap all the available details of best gaming laptops from digit.in.\n",
    "\n",
    "from selenium import webdriver\n",
    "from BeautifulSoup import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "driver = webdriver.Chrome(\"/usr/lib/chromium-browser/chromedriver\")\n",
    "\n",
    "Operating Systems=[] #List to store Details of the operating system\n",
    "Displays=[] #List to store the details of the Display\n",
    "Processors=[] #List to store the details of processor\n",
    "driver.get(\"https://www.digit.in/top-products/best-gaming-laptops-40.html\")\n",
    "content = driver.page_source\n",
    "soup = BeautifulSoup(content)\n",
    "for a in soup.findAll('a',href=True, attrs={'class':'_31qSD5'}):\n",
    "OS=a.find('div', attrs={'class':'_3wU53n'})\n",
    "Display=a.find('div', attrs={'class':'_1vC4OE _2rQ-NK'})\n",
    "Processor=a.find('div', attrs={'class':'hGSR34 _2beYZw'})\n",
    "Operating Systems.append(name.text)\n",
    "Displays.append(price.text)\n",
    "Processors.append(rating.text)\n",
    "\n",
    "python web-s.py\n",
    "df = pd.DataFrame({'OS Name':Operating Systems,'Display':Displays,'Processor':Processors})df.to_csv('products.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4d9463",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Write a python program to scrape the details for all billionaires from www.forbes.com. Details to be scrapped: “Rank”, “Name”, “Net worth”, “Age”, “Citizenship”, “Source”, “Industry”.\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pandas.util\n",
    "forbes = pd.read_csv('reviews_static.csv', names = ['Rank', 'Name', 'Net Worth', 'Age', 'Source', 'Country'])\n",
    "\n",
    "forbes_realtime = pd.read_csv('reviews_realtime.csv', names = ['Rank', 'Name', 'Net Worth', 'Age', 'Source', 'Country'])\n",
    "\n",
    "forbes_women = pd.read_csv('reviews_static_women.csv', names = ['Rank', 'Name', 'Net Worth', 'Age', 'Source', 'Country'])\n",
    "#creating data frame for current net worth\n",
    "realtime = forbes_realtime[['Name','Net Worth']]\n",
    "\n",
    "#merging dataframe\n",
    "forbes_data = pd.merge(forbes, realtime,how = 'left', on = 'Name')\n",
    "\n",
    "#cleaning NA values\n",
    "forbes_data['Age'] = forbes_data['Age'].fillna(forbes_data['Age'].median())\n",
    "forbes_data['Net Worth_y'] = forbes_data['Net Worth_y'].fillna(0)\n",
    "\n",
    "#creating new column \"Change\"\n",
    "forbes_data['Change'] = forbes_data['Net Worth_y']- forbes_data['Net Worth_x']\n",
    "# changing float into integar\n",
    "forbes_data['Age'] = list(map(lambda x: int(x), forbes_data['Age']))\n",
    "forbes_data['Rank'] = list(map(lambda x: int(x), forbes_data['Rank']))\n",
    "#creating women data frame, so i can merge that into main data frame to create Gender column\n",
    "women = forbes_women['Name']\n",
    "\n",
    "#Creating new column \"Gender\" in main data frame\n",
    "forbes_data['Gender'] = forbes_data['Name'].isin(women)\n",
    "forbes_data['Gender'].replace([True,False],['F','M'],inplace = True)\n",
    "#checking final data frame\n",
    "forbes_data.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8990e607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Write a program to extract at least 500 Comments, Comment upvote and time when comment was posted from any YouTube Video.\n",
    "\n",
    "import requests\n",
    "import json\n",
    "import time\n",
    "\n",
    "\n",
    "def search_dict(partial, key):\n",
    "    \"\"\"\n",
    "    A handy function that searches for a specific `key` in a `data` dictionary/list\n",
    "    \"\"\"\n",
    "    if isinstance(partial, dict):\n",
    "        for k, v in partial.items():\n",
    "            if k == key:\n",
    "                # found the key, return the value\n",
    "                yield v\n",
    "            else:\n",
    "                # value of the dict may be another dict, so we search there again\n",
    "                for o in search_dict(v, key):\n",
    "                    yield o\n",
    "    elif isinstance(partial, list):\n",
    "        # if the passed data is a list\n",
    "        # iterate over it & search for the key at the items in the list\n",
    "        for i in partial:\n",
    "            for o in search_dict(i, key):\n",
    "                yield o\n",
    "\n",
    "\n",
    "def find_value(html, key, num_sep_chars=2, separator='\"'):\n",
    "    # define the start position by the position of the key + \n",
    "    # length of key + separator length (usually : and \")\n",
    "    start_pos = html.find(key) + len(key) + num_sep_chars\n",
    "    # the end position is the position of the separator (such as \")\n",
    "    # starting from the start_pos\n",
    "    end_pos = html.find(separator, start_pos)\n",
    "    # return the content in this range\n",
    "    return html[start_pos:end_pos]\n",
    "\n",
    "\n",
    "def get_comments(url):\n",
    "    session = requests.Session()\n",
    "    # make the request\n",
    "    res = session.get(url)\n",
    "    # extract the XSRF token\n",
    "    xsrf_token = find_value(res.text, \"XSRF_TOKEN\", num_sep_chars=3)\n",
    "    # parse the YouTube initial data in the <script> tag\n",
    "    data_str = find_value(res.text, 'window[\"ytInitialData\"] = ', num_sep_chars=0, separator=\"\\n\").rstrip(\";\")\n",
    "    # convert to Python dictionary instead of plain text string\n",
    "    data = json.loads(data_str)\n",
    "    # search for the ctoken & continuation parameter fields\n",
    "    for r in search_dict(data, \"itemSectionRenderer\"):\n",
    "        pagination_data = next(search_dict(r, \"nextContinuationData\"))\n",
    "        if pagination_data:\n",
    "            # if we got something, break out of the loop,\n",
    "            # we have the data we need\n",
    "            break\n",
    "\n",
    "    continuation_tokens = [(pagination_data['continuation'], pagination_data['clickTrackingParams'])]\n",
    "\n",
    "    while continuation_tokens:\n",
    "        # keep looping until continuation tokens list is empty (no more comments)\n",
    "        continuation, itct = continuation_tokens.pop()\n",
    "    \n",
    "        # construct params parameter (the ones in the URL)\n",
    "        params = {\n",
    "            \"action_get_comments\": 1,\n",
    "            \"pbj\": 1,\n",
    "            \"ctoken\": continuation,\n",
    "            \"continuation\": continuation,\n",
    "            \"itct\": itct,\n",
    "        }\n",
    "\n",
    "        # construct POST body data, which consists of the XSRF token\n",
    "        data = {\n",
    "            \"session_token\": xsrf_token,\n",
    "        }\n",
    "\n",
    "        # construct request headers\n",
    "        headers = {\n",
    "            \"x-youtube-client-name\": \"1\",\n",
    "            \"x-youtube-client-version\": \"2.20200731.02.01\"\n",
    "        }\n",
    "\n",
    "        # make the POST request to get the comments data\n",
    "        response = session.post(\"https://www.youtube.com/comment_service_ajax\", params=params, data=data, headers=headers)\n",
    "        # convert to a Python dictionary\n",
    "        comments_data = json.loads(response.text)\n",
    "\n",
    "        for comment in search_dict(comments_data, \"commentRenderer\"):\n",
    "            # iterate over loaded comments and yield useful info\n",
    "            yield {\n",
    "                \"commentId\": comment[\"commentId\"],\n",
    "                \"text\": ''.join([c['text'] for c in comment['contentText']['runs']]),\n",
    "                \"time\": comment['publishedTimeText']['runs'][0]['text'],\n",
    "                \"isLiked\": comment[\"isLiked\"],\n",
    "                \"likeCount\": comment[\"likeCount\"],\n",
    "                # \"replyCount\": comment[\"replyCount\"],\n",
    "                'author': comment.get('authorText', {}).get('simpleText', ''),\n",
    "                'channel': comment['authorEndpoint']['browseEndpoint']['browseId'],\n",
    "                'votes': comment.get('voteCount', {}).get('simpleText', '0'),\n",
    "                'photo': comment['authorThumbnail']['thumbnails'][-1]['url'],\n",
    "                \"authorIsChannelOwner\": comment[\"authorIsChannelOwner\"],\n",
    "            }\n",
    "\n",
    "        # load continuation tokens for next comments (ctoken & itct)\n",
    "        continuation_tokens = [(next_cdata['continuation'], next_cdata['clickTrackingParams'])\n",
    "                         for next_cdata in search_dict(comments_data, 'nextContinuationData')] + continuation_tokens\n",
    "\n",
    "        # avoid heavy loads with popular videos\n",
    "        time.sleep(0.1)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # from pprint import pprint\n",
    "    # url = \"https://www.youtube.com/watch?v=jNQXAC9IVRw\"\n",
    "    # for count, comment in enumerate(get_comments(url)):\n",
    "    #     if count == 3:\n",
    "    #         break\n",
    "    #     pprint(comment)\n",
    "    #     print(\"=\"*50)\n",
    "    import argparse\n",
    "    import os\n",
    "\n",
    "    parser = argparse.ArgumentParser(description=\"Simple YouTube Comment extractor\")\n",
    "    parser.add_argument(\"url\", help=\"The YouTube video full URL\")\n",
    "    parser.add_argument(\"-l\", \"--limit\", type=int, help=\"Number of maximum comments to extract, helpful for longer videos\")\n",
    "    parser.add_argument(\"-o\", \"--output\", help=\"Output JSON file, e.g data.json\")\n",
    "\n",
    "    # parse passed arguments\n",
    "    args = parser.parse_args()\n",
    "    limit = args.limit\n",
    "    output = args.output\n",
    "    url = args.url\n",
    "\n",
    "    from pprint import pprint\n",
    "    for count, comment in enumerate(get_comments(url)):\n",
    "        if limit and count >= limit:\n",
    "            # break out of the loop when we exceed limit specified\n",
    "            break\n",
    "        if output:\n",
    "            # write comment as JSON to a file\n",
    "            with open(output, \"a\") as f:\n",
    "                # begin writing, adding an opening brackets\n",
    "                if count == 0:\n",
    "                    f.write(\"[\")\n",
    "                f.write(json.dumps(comment, ensure_ascii=False) + \",\")\n",
    "        else:\n",
    "            pprint(comment)\n",
    "            print(\"=\"*50)\n",
    "    print(\"total comments extracted:\", count)\n",
    "    if output:\n",
    "        # remove the last comma ','\n",
    "        with open(output, \"rb+\") as f:\n",
    "            f.seek(-1, os.SEEK_END)\n",
    "            f.truncate()\n",
    "        # add \"]\" to close the list in the end of the file\n",
    "        with open(output, \"a\") as f:\n",
    "            print(\"]\", file=f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e1d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Write a python program to scrape a data for all available Hostels from https://www.hostelworld.com/ in “London” location.\n",
    "\n",
    "import selenium\n",
    "import selenium.webdriver\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "data = []\n",
    "\n",
    "final_list = [\n",
    "    'https://www.hostelworld.com/hostels/London'\n",
    "]\n",
    "\n",
    "# load your driver only once to save time\n",
    "driver = selenium.webdriver.Chrome()\n",
    "\n",
    "for url in final_list:\n",
    "    data.append({})\n",
    "\n",
    "    # cache the HTML code to the filesystem\n",
    "    # generate a filename from the URL where all non-alphanumeric characters (e.g. :/) are replaced with underscores _\n",
    "    filename = ''.join([s if s.isalnum() else '_' for s in url])\n",
    "    if not os.path.isfile(filename):\n",
    "        driver.get(url)\n",
    "        \n",
    "        # better use selenium's wait functions here  \n",
    "        time.sleep(random.randint(10, 20))\n",
    "        source = driver.page_source\n",
    "        with open(filename, 'w', encoding='utf-8') as f:\n",
    "            f.write(source)\n",
    "    else:\n",
    "        with open(filename, 'r', encoding='utf-8') as f:\n",
    "            source = f.read()\n",
    "    soup = BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "    review = soup.find_all(class_='reviews')[-1]\n",
    "    \n",
    "    try:\n",
    "        price = soup.find_all('span', attrs={'class':'price'})[-1] \n",
    "    except:\n",
    "        price = soup.find_all('span', attrs={'class':'price'})\n",
    "\n",
    "    data[-1]['name'] = soup.find_all(class_=['title-2'])[0].text.strip()\n",
    "    \n",
    "    rating_labels = soup.find_all(class_=['rating-label body-3'])\n",
    "    rating_scores = soup.find_all(class_=['rating-score body-3'])\n",
    "    assert len(rating_labels) == len(rating_scores)\n",
    "    for label, score in zip(rating_labels, rating_scores):\n",
    "        data[-1][label.text.strip()] = score.text.strip()\n",
    "    \n",
    "    data[-1]['price'] = price.text.strip()\n",
    "    data[-1]['review'] = review.text.strip()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
