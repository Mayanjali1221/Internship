{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3989f08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1. Scrape the details of most viewed videos on YouTube from Wikipedia.\n",
    "Url = https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\n",
    "You need to find following details:\n",
    "A) Rank\n",
    "B) Name\n",
    "C) Artist\n",
    "D) Upload date\n",
    "E) Views\n",
    "\n",
    "# importing the libraries\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "# Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException,\n",
    "NoSuchElementException\n",
    "#Importing requests\n",
    "import requests\n",
    "# importing regex\n",
    "import re\n",
    "# creating function\n",
    "def scrape_info(url):\n",
    "\t\n",
    "\t# getting the request from url\n",
    "\tr = requests.get(url)\n",
    "\t\n",
    "\t# converting the text\n",
    "\ts = BeautifulSoup(r.text, \"html.parser\")\n",
    "\t\n",
    "\t# finding meta info for title\n",
    "\tRank = s.find(\"span\", class_=\"Rank\").text.replace(\"\\n\", \"\")\n",
    "    \n",
    "    Name = s.find(\"span\", class_=\"Name\").text.replace(\"\\n\", \"\")\n",
    "    \n",
    "    Artist = s.find(\"span\", class_=\"Artist\").text.replace(\"\\n\", \"\")\n",
    "    \n",
    "    Upload Date = s.find(\"span\", class_=\"Upload Date\").text.replace(\"\\n\", \"\")\n",
    "    \n",
    "    # finding meta info for views\n",
    "\tviews = s.find(\"div\", class_=\"watch-view-count\").text\n",
    "\n",
    "\t# saving this data in dictionary\n",
    "\tdata = {'Rank':Rank, 'Name':Name, 'Artist':Artist, 'Upload Date':Upload Date, 'Views':Views}\n",
    "\t\n",
    "\t# returning the dictionary\n",
    "\treturn data\n",
    "\n",
    "# main function\n",
    "if __name__ == \"__main__\":\n",
    "\t\n",
    "\t# URL of the video\n",
    "\turl =\" https://en.wikipedia.org/wiki/List_of_most-viewed_YouTube_videos\"\n",
    "\t\n",
    "\t# calling the function\n",
    "\tdata = scrape_info(url)\n",
    "\t\n",
    "\t# printing the dictionary\n",
    "\tprint(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6559d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Scrape the details team Indiaâ€™s international fixtures from bcci.tv.\n",
    "Url = https://www.bcci.tv/.\n",
    "You need to find following details:\n",
    "A) Match title (I.e. 1\n",
    "st ODI)\n",
    "B) Series\n",
    "C) Place\n",
    "D) Date\n",
    "E) Time\n",
    "Note: - From bcci.tv home page you have reach to the international fixture page through code\n",
    "    \n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "# Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException,\n",
    "NoSuchElementException\n",
    "#Importing requests\n",
    "import requests\n",
    "# importing regex\n",
    "import re\n",
    "\n",
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)\n",
    "# Opening the forbes.com\n",
    "url = \"https://www.bcci.tv/international/fixtures\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "#clicking the fixtures button\n",
    "button = driver.find_element_by_xpath(\"//button[@class='icon--\n",
    "hamburger']\")\n",
    "button.click()\n",
    "time.sleep(1)\n",
    "#select International fixtures\n",
    "International fixtures =\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/\n",
    "li[1]\")\n",
    "International fixtures.click()\n",
    "time.sleep(1)\n",
    "#select India International fixtures\n",
    "India_International_fixtures=\n",
    "driver.find_element_by_xpath(\"/html/body/div[1]/header/nav/div[3]/ul/\n",
    "li[1]/div[2]/ul/li[2]/a\")\n",
    "India_International_fixtures.click()\n",
    "time.sleep(1)\n",
    "#make empty listes\n",
    "Match_title = []\n",
    "Series = []\n",
    "Place= []\n",
    "Date= []\n",
    "Times = []\n",
    "                             \n",
    "while(True):\n",
    " #scraping Match title of India International fixtures\n",
    " Match_title= driver.find_elements_by_xpath(\"//div[@class='Match_title']\")\n",
    " for i in Match_title:\n",
    " Match_title.append(i.text)\n",
    " time.sleep(1)\n",
    "\n",
    " #scraping Series of India International fixtures\n",
    " Series=\n",
    "driver.find_elements_by_xpath(\"//div[@class='Series']//div\")\n",
    " for i in Series:\n",
    " Series.append(i.text)\n",
    " time.sleep(1)\n",
    "\n",
    " #scraping Place of India International fixtures\n",
    " Place= driver.find_elements_by_xpath(\"//div[@class='Place']//div\")\n",
    " for i in Place:\n",
    " Place.append(i.text)\n",
    " time.sleep(1)\n",
    "\n",
    " #scraping Date of India International fixtures\n",
    " Date=\n",
    "driver.find_elements_by_xpath(\"//div[@class='Date']\")\n",
    " for i in Date:\n",
    " Date.append(i.text)\n",
    " time.sleep(1)\n",
    "\n",
    " #scraping Time of India International fixtures\n",
    " Times= driver.find_elements_by_xpath(\"//div[@class='Times']\")\n",
    " for i in Times:\n",
    " Times.append(i.text)\n",
    " time.sleep(1)\n",
    "\n",
    " try:\n",
    " next_button =\n",
    "driver.find_element_by_xpath(\"//button[@class='pagination-btn\n",
    "pagination-btn--next ']\")\n",
    " next_button.click()\n",
    " except:\n",
    " break \n",
    "                             \n",
    "time.sleep(2)\n",
    "#creating dataframe\n",
    "df=pd.DataFrame({'Match_title':Match_title,\n",
    " 'Series':Series,\n",
    " 'Place':Place,\n",
    " 'Date':Date,\n",
    " 'Times':Times})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae16f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Scrape the details of selenium exception from guru99.com.\n",
    "Url = https://www.guru99.com/\n",
    "You need to find following details:\n",
    "A) Name\n",
    "B) Description\n",
    "Note: - From guru99 home page you have to reach to selenium exception handling page through code.\n",
    "    \n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "# Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException,\n",
    "NoSuchElementException\n",
    "#Importing requests\n",
    "import requests\n",
    "# importing regex\n",
    "import re\n",
    "\n",
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)\n",
    "# Opening the Bcci.com\n",
    "url = \"= https://www.guru99.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "#clicking the Testing button\n",
    "button = driver.find_element_by_xpath(\"//*[@id=\"menu-item-3173\"]/a\")\n",
    "button.click()\n",
    "time.sleep(1)\n",
    "\n",
    "#clicking the Selenium button\n",
    "Sbutton = driver.find_element_by_xpath(\"//*[@id=\"menu-item-4622\"]/a\")\n",
    "Sbutton.click()\n",
    "time.sleep(1)\n",
    "\n",
    "#clicking on Tutorial tab\n",
    "Tab_click = driver.find_element_by_xpath(\"//*[@id=\"post-193\"]/div/div/table[5]/tbody/tr[34]/td[1]/a\")\n",
    "Tab_click.click()\n",
    "\n",
    "while(True):\n",
    " #scraping Exception Name\n",
    " Name= driver.find_elements_by_xpath(\"//*[@id=\"post-1953\"]/div/div/table/tbody/tr[1]/th[1]\")\n",
    " for i in Name:\n",
    " Name.append(i.text)\n",
    " time.sleep(1)\n",
    "\n",
    " #scraping Exception Description\n",
    " Description = driver.find_elements_by_xpath(\"//*[@id=\"post-1953\"]/div/div/table/tbody/tr[1]/th[2]\")\n",
    " for i in Description:\n",
    " Description.append(i.text)\n",
    " time.sleep(1)\n",
    "    \n",
    " #creating dataframe\n",
    "df=pd.DataFrame({'Name':Name,\n",
    " 'Description':Description,})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7b52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Scrape the details of State-wise GDP of India from statisticstime.com.\n",
    "#Url = http://statisticstimes.com/\n",
    "#You have to find following details:\n",
    "#A) Rank\n",
    "#B) State\n",
    "#C) GSDP\n",
    "#D) GSDP\n",
    "#E) Share\n",
    "#F) GDP($ billion)\n",
    "# Note: - From statisticstimes home page you have to reach to economy page through code.\n",
    "    \n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "# Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException,\n",
    "NoSuchElementException\n",
    "#Importing requests\n",
    "import requests\n",
    "# importing regex\n",
    "import re\n",
    "\n",
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)\n",
    "# Opening the statisticstimes.com\n",
    "url = \"http://statisticstimes.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "#clicking the Economy button\n",
    "button = driver.find_element_by_xpath(\"//*[@id=\"top\"]/div[2]/div[2]/button\")\n",
    "button.click()\n",
    "time.sleep(1)\n",
    "\n",
    "#click on India from drop down menu\n",
    "Ibutton = driver.find_element_by_xpath(\"//*[@id=\"top\"]/div[2]/div[2]/div/a[3]\")\n",
    "Ibutton.click()\n",
    "time.sleep(1)\n",
    "\n",
    "#clicking on a link tab of GDP in Indian states\n",
    "Tab_click = driver.find_element_by_xpath(\"/html/body/div[2]/div[2]/div[2]/ul/li[1]/a\")\n",
    "Tab_click.click()\n",
    "\n",
    "#make empty listes\n",
    "Rank = []\n",
    "State = []\n",
    "GSDP = []\n",
    "GSDP2 = []\n",
    "Share = []\n",
    "GDP = []\n",
    "\n",
    "\n",
    "while(True):\n",
    " #scraping Rank Details\n",
    " rank= driver.find_elements_by_xpath(\"//*[@id=\"table_id\"]/thead/tr[1]/th[1]\")\n",
    " for i in rank:\n",
    " Rank.append(i.text)\n",
    " time.sleep(1)\n",
    "\n",
    " #scraping State Details\n",
    " state = driver.find_elements_by_xpath(\"//*[@id=\"table_id\"]/thead/tr[1]/th[2]\")\n",
    " for i in state:\n",
    " State.append(i.text)\n",
    " time.sleep(1)\n",
    "    \n",
    " #scraping GSDP (Cr INR at Current prices) Details\n",
    " gsdp = driver.find_elements_by_xpath(\"//*[@id=\"table_id\"]/thead/tr[1]/th[3]\")\n",
    " for i in gsdp:\n",
    " GSDP.append(i.text)\n",
    " time.sleep(1)\n",
    "    \n",
    " #scraping GSDP (Cr INR at 2011-12 prices Details\n",
    " gsdp2 = driver.find_elements_by_xpath(\"//*[@id=\"table_id\"]/thead/tr[1]/th[6]\")\n",
    " for i in gsdp2:\n",
    " GSDP2.append(i.text)\n",
    " time.sleep(1)\n",
    "    \n",
    " #scraping Share Details\n",
    " share = driver.find_elements_by_xpath(\"//*[@id=\"table_id\"]/thead/tr[1]/th[4]\")\n",
    " for i in share:\n",
    " Share.append(i.text)\n",
    " time.sleep(1)  \n",
    "    \n",
    "    \n",
    " #scraping GDP Details\n",
    " gdp = driver.find_elements_by_xpath(\"//*[@id=\"table_id\"]/thead/tr[1]/th[5]\")\n",
    " for i in gdp:\n",
    " GDP.append(i.text)\n",
    " time.sleep(1) \n",
    "    \n",
    " #creating dataframe\n",
    " df=pd.DataFrame({'Rank':Rank,\n",
    " 'State':State,\n",
    " 'GSDP':GSDP,\n",
    " 'GSDP2':GSDP2, \n",
    " 'Share':Share,\n",
    " 'GDP':GDP })\n",
    "    \n",
    "Df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938ffbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5. Scrape the details of trending repositories on Github.com.\n",
    "#Url = https://github.com/\n",
    "#You have to find the following details:\n",
    "#A) Repository title\n",
    "#B) Repository description\n",
    "#C) Contributors count\n",
    "#D) Language used\n",
    "\n",
    "#Note: - From the home page you have to click on the trending option from Explore menu through code.\n",
    "\n",
    "import selenium\n",
    "import pandas as pd\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "# Importing selenium webdriver\n",
    "from selenium import webdriver\n",
    "# Importing required Exceptions which needs to handled\n",
    "from selenium.common.exceptions import StaleElementReferenceException,\n",
    "NoSuchElementException\n",
    "#Importing requests\n",
    "import requests\n",
    "# importing regex\n",
    "import re\n",
    "\n",
    "# Activating the chrome browser\n",
    "driver=webdriver.Chrome(\"chromedriver.exe\")\n",
    "time.sleep(2)\n",
    "# Opening the statisticstimes.com\n",
    "url = \"= https://github.com/\"\n",
    "driver.get(url)\n",
    "time.sleep(2)\n",
    "\n",
    "#clicking the Explore button\n",
    "button = driver.find_element_by_xpath(\"/html/body/div[1]/header/div[3]/nav/a[4]\")\n",
    "button.click()\n",
    "time.sleep(1)\n",
    "\n",
    "#click on Trending button\n",
    "Ibutton = driver.find_element_by_xpath(\"//*[@id=\"js-pjax-container\"]/div[1]/nav/div/a[3]\")\n",
    "Ibutton.click()\n",
    "time.sleep(1)\n",
    "\n",
    "#make empty listes\n",
    "Repository title = []\n",
    "Repository description = []\n",
    "Contributors count = []\n",
    "Language used = []\n",
    "\n",
    "while(True):\n",
    " #scraping Repository title Details\n",
    "repository title= driver.find_elements_by_xpath(\"//*[@id=\"js-pjax-container\"]/div[3]/div/div[2]/article[1]/h1/a/span\")\n",
    "for i in repository title:\n",
    "Repository title.append(i.text)\n",
    " time.sleep(1)\n",
    "\n",
    " #scraping Repository description Details\n",
    " repository description = driver.find_elements_by_xpath(\"//*[@id=\"js-pjax-container\"]/div[3]/div/div[2]/article[1]/p\")\n",
    "    for i in repository description:\n",
    "Repository description.append(i.text)\n",
    " time.sleep(1)\n",
    "    \n",
    " #scraping Contributors count Details\n",
    " contributors count = driver.find_elements_by_xpath(\"//*[@id=\"js-pjax-container\"]/div[3]/div/div[2]/article[1]/div[2]/span[1]/span[2]]\")\n",
    " for i in contributors count:\n",
    "Contributors count.append(i.text)\n",
    " time.sleep(1)\n",
    "    \n",
    " #scraping Language used Details\n",
    " language used = driver.find_elements_by_xpath(\"//*[@id=\"js-pjax-container\"]/div[3]/div/div[2]/article[1]/div[2]/span[1]/span[2]\")\n",
    " for i in language used:\n",
    "Language used.append(i.text)\n",
    " time.sleep(1)\n",
    "    \n",
    "    #creating dataframe\n",
    " df=pd.DataFrame({'Repository title':Repository title,\n",
    " 'Repository description ':Repository description,\n",
    " 'Contributors count':Contributors count,\n",
    " 'Language used':Language used })\n",
    "    \n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
